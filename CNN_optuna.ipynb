{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap52v7uXnWRe",
        "outputId": "b0b80757-0bb3-4007-f4e0-ff3a9a1ef7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "_NvdLAdMnf00"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQhqb5YOopnC",
        "outputId": "fb67b7d6-521b-471e-bf9d-304deb6a1166"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "file_path = \"fashion-mnist_train.csv\"\n",
        "\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"zalando-research/fashionmnist\",\n",
        "  file_path\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE1CblROo1Bj",
        "outputId": "91a603cd-0076-4ab7-a8ee-14b80ac63548"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1182604528.py:6: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'fashionmnist' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = df.iloc[:, 1:].values, df.iloc[:, 0].values"
      ],
      "metadata": {
        "id": "VUOqZYD6pB3G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split(x,y, shuffle=True, random_state=42, stratify = y)"
      ],
      "metadata": {
        "id": "ZxkX4UmOpLk0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_augment = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip()\n",
        "])"
      ],
      "metadata": {
        "id": "3F0aYcmKpSKa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, labels, transform=None):\n",
        "    self.features = torch.tensor(features, dtype=torch.float).reshape(-1, 1, 28, 28)\n",
        "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img, labels = self.features[idx], self.labels[idx]\n",
        "    if self.transform:\n",
        "      img =  self.transform(img)\n",
        "    return img, labels\n"
      ],
      "metadata": {
        "id": "qfkGovTppu6z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traindataset = CustomDataset(xtrain, ytrain, transform=train_augment)\n",
        "testdataset = CustomDataset(xtest, ytest)"
      ],
      "metadata": {
        "id": "FPh11Vkj1bAZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_features, filter_per_layer, filter_size, activation, neurons_per_layer, dropout_per_layer):\n",
        "    super(myCNN, self).__init__()\n",
        "    feature_layers = []\n",
        "\n",
        "    activation = getattr(nn, activation)\n",
        "    for filters, size in zip(filter_per_layer, filter_size):\n",
        "      feature_layers.append(nn.Conv2d(input_features, filters, size, padding='same'))\n",
        "      feature_layers.append(nn.BatchNorm2d(filters))\n",
        "      feature_layers.append(activation())\n",
        "      feature_layers.append(nn.MaxPool2d(2,2))\n",
        "      input_features = filters\n",
        "\n",
        "    self.features = nn.Sequential(*feature_layers)\n",
        "\n",
        "    dummy_input = torch.zeros(1, 1, 28, 28)\n",
        "    with torch.no_grad():\n",
        "      dummy_output = self.features(dummy_input)\n",
        "    input_size = dummy_output.flatten().shape[0]\n",
        "\n",
        "    classification_layers = [nn.Flatten(), ]\n",
        "\n",
        "    for neuron, dropout in zip(neurons_per_layer, dropout_per_layer):\n",
        "      classification_layers.append(nn.Linear(input_size, neuron))\n",
        "      classification_layers.append(nn.BatchNorm1d(neuron))\n",
        "      classification_layers.append(activation())\n",
        "      classification_layers.append(nn.Dropout(dropout))\n",
        "      input_size = neuron\n",
        "\n",
        "    classification_layers.append(nn.Linear(input_size, 10))\n",
        "\n",
        "    self.classifier = nn.Sequential(*classification_layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "DL_xrliL1uHN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "  num_epochs = trial.suggest_int('num_epochs', 1, 4)\n",
        "  learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
        "  batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512])\n",
        "  optimizer = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
        "  num_conv_layers = trial.suggest_categorical('conv_layers', [1,2,3])\n",
        "  activation = trial.suggest_categorical('activation', ['ReLU', 'Tanh', 'ELU'])\n",
        "  num_fc_layers = trial.suggest_categorical('num_fc_layers', [2, 3, 4, 5, 6])\n",
        "\n",
        "  filter_per_layer = []\n",
        "  filter_size = []\n",
        "\n",
        "  for i in range(num_conv_layers):\n",
        "    filter_per_layer.append(trial.suggest_int(f\"filter_in_conv_layer_{i}\", 16, 128))\n",
        "    filter_size.append(trial.suggest_int(f'filter_size_in_conv_layer_{i}', 2, 5))\n",
        "\n",
        "  neurons_per_layer = []\n",
        "  dropout_per_layer = []\n",
        "  for i in range(num_fc_layers):\n",
        "    neurons_per_layer.append(trial.suggest_categorical(f'neurons_in_fc_layer_{i}', [16, 32, 64, 128, 256]))\n",
        "    dropout_per_layer.append(trial.suggest_categorical(f'dropout_in_fc_layer_{i}', [0.1, 0.2, 0.3, 0.4]))\n",
        "\n",
        "  model = myCNN(input_features=1, filter_per_layer=filter_per_layer, filter_size=filter_size, activation = activation, neurons_per_layer=neurons_per_layer, dropout_per_layer=dropout_per_layer)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  train_loader = DataLoader(traindataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "  test_loader = DataLoader(traindataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer_class = getattr(optim, optimizer)\n",
        "\n",
        "  optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "\n",
        "      batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "      pred = model(batch_features)\n",
        "\n",
        "      loss = loss_function(pred, batch_labels)\n",
        "\n",
        "      epoch_loss.append(loss.item())\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    prediction = model(testdataset.features.to(device))\n",
        "    prediction = torch.argmax(prediction, dim=1)\n",
        "    score = accuracy_score(testdataset.labels.cpu(), prediction.cpu())\n",
        "\n",
        "  return score\n"
      ],
      "metadata": {
        "id": "fmKRHWBB2IqD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pruner = optuna.pruners.MedianPruner()\n",
        "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "study.optimize(objective, n_trials=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI5Qz85r4m_M",
        "outputId": "8850de1e-781c-455a-a6e4-586952908a37"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-05 17:43:31,412] A new study created in memory with name: no-name-31c62a64-af71-4239-a380-d1ffa0aeb2fe\n",
            "[I 2026-01-05 17:44:54,316] Trial 0 finished with value: 0.8702 and parameters: {'num_epochs': 4, 'learning_rate': 0.0013033522556026179, 'batch_size': 256, 'optimizer': 'RMSprop', 'conv_layers': 1, 'activation': 'Tanh', 'num_fc_layers': 5, 'filter_in_conv_layer_0': 87, 'filter_size_in_conv_layer_0': 4, 'neurons_in_fc_layer_0': 64, 'dropout_in_fc_layer_0': 0.1, 'neurons_in_fc_layer_1': 64, 'dropout_in_fc_layer_1': 0.4, 'neurons_in_fc_layer_2': 128, 'dropout_in_fc_layer_2': 0.3, 'neurons_in_fc_layer_3': 32, 'dropout_in_fc_layer_3': 0.4, 'neurons_in_fc_layer_4': 32, 'dropout_in_fc_layer_4': 0.3}. Best is trial 0 with value: 0.8702.\n",
            "[I 2026-01-05 17:45:13,573] Trial 1 finished with value: 0.7990666666666667 and parameters: {'num_epochs': 1, 'learning_rate': 7.648678986625903e-05, 'batch_size': 64, 'optimizer': 'RMSprop', 'conv_layers': 2, 'activation': 'ELU', 'num_fc_layers': 2, 'filter_in_conv_layer_0': 26, 'filter_size_in_conv_layer_0': 5, 'filter_in_conv_layer_1': 17, 'filter_size_in_conv_layer_1': 4, 'neurons_in_fc_layer_0': 16, 'dropout_in_fc_layer_0': 0.3, 'neurons_in_fc_layer_1': 256, 'dropout_in_fc_layer_1': 0.2}. Best is trial 0 with value: 0.8702.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfTJoBLsO56o",
        "outputId": "b70f60ea-fa61-4a25-a991-f92be1da348b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_epochs': 4,\n",
              " 'learning_rate': 0.0013033522556026179,\n",
              " 'batch_size': 256,\n",
              " 'optimizer': 'RMSprop',\n",
              " 'conv_layers': 1,\n",
              " 'activation': 'Tanh',\n",
              " 'num_fc_layers': 5,\n",
              " 'filter_in_conv_layer_0': 87,\n",
              " 'filter_size_in_conv_layer_0': 4,\n",
              " 'neurons_in_fc_layer_0': 64,\n",
              " 'dropout_in_fc_layer_0': 0.1,\n",
              " 'neurons_in_fc_layer_1': 64,\n",
              " 'dropout_in_fc_layer_1': 0.4,\n",
              " 'neurons_in_fc_layer_2': 128,\n",
              " 'dropout_in_fc_layer_2': 0.3,\n",
              " 'neurons_in_fc_layer_3': 32,\n",
              " 'dropout_in_fc_layer_3': 0.4,\n",
              " 'neurons_in_fc_layer_4': 32,\n",
              " 'dropout_in_fc_layer_4': 0.3}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0OD-sfWP0hR",
        "outputId": "820d3b74-f58a-4f13-d44b-161c4c3a3208"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8702"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1CfiE8IxRBaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}