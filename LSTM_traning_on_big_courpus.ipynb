{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xfho0tGZXvlE",
        "outputId": "0fcc7bd8-0fa4-4d5e-c38a-bfbd4bd39c90"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20251230 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251230 pdfplumber-0.11.9 pypdfium2-5.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3kIwSadjXasD"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "7yTGeubeAQ3R",
        "outputId": "76c286c6-1027-414d-a357-3508ca613e96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrc7hfkTf0cH",
        "outputId": "5b361f67-d148-436e-ec74-b46052083765"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "books = ['/content/Deep+Learning+Ian+Goodfellow.pdf', '/content/Free PDF - Generative AI Foundations in Python.pdf',\n",
        "         '/content/asset-v1_ColumbiaX+CSMM.101x+1T2017+type@asset+block@AI_edx_ml_5.1intro.pdf',\n",
        "         '/content/deep-learning-material-dept-ece-ase-blr-1.pdf','thebook.pdf', '/content/mlops.pdf']"
      ],
      "metadata": {
        "id": "VfMwwa8LbzRB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = ''\n",
        "\n",
        "for book in books:\n",
        "  with pdfplumber.open(book) as pdf:\n",
        "    for page in pdf.pages:\n",
        "      full_text = full_text + page.extract_text() + '\\n'\n",
        "    print(book, 'is read completely')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKXNzTAkX3P2",
        "outputId": "b4d9750d-dcb3-4b61-b0ef-2d058a97afe6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Deep+Learning+Ian+Goodfellow.pdf is read completely\n",
            "/content/Free PDF - Generative AI Foundations in Python.pdf is read completely\n",
            "/content/asset-v1_ColumbiaX+CSMM.101x+1T2017+type@asset+block@AI_edx_ml_5.1intro.pdf is read completely\n",
            "/content/deep-learning-material-dept-ece-ase-blr-1.pdf is read completely\n",
            "thebook.pdf is read completely\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mlops.pdf is read completely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = full_text.lower()"
      ],
      "metadata": {
        "id": "I88NAA3IjUvA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(full_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjcIezDNYDkp",
        "outputId": "c7e94ad8-3966-4f37-8421-fb723093377b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3841861"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(full_text)"
      ],
      "metadata": {
        "id": "ZAKvhfB1fB-h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'<UNK>' : 0}"
      ],
      "metadata": {
        "id": "3eeJNWUYgFh0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in set(tokens):\n",
        "  if token not in vocab:\n",
        "    vocab[token] = len(vocab)"
      ],
      "metadata": {
        "id": "jcFTF6oXgPDR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEyBMW7KhojU",
        "outputId": "7e2662a6-8ca1-442b-883f-ababffe338c0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28321"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = full_text.split('\\n')"
      ],
      "metadata": {
        "id": "5KbH16OBhyCx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text, vocab = vocab):\n",
        "  numerical_text = []\n",
        "  for token in text:\n",
        "    if token in vocab:\n",
        "      numerical_text.append(vocab[token])\n",
        "    else:\n",
        "      numerical_text.append(vocab['<UNK>'])\n",
        "\n",
        "  return numerical_text"
      ],
      "metadata": {
        "id": "3a3aKyWNh7EH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_numerical_sentence = []\n",
        "\n",
        "for sentence in input_sentence:\n",
        "  input_numerical_sentence.append(text_to_indices(word_tokenize(sentence)))"
      ],
      "metadata": {
        "id": "o1r_i3KKiVYS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_to_remove = []\n",
        "\n",
        "for i in range(len(input_numerical_sentence)):\n",
        "  if len(input_numerical_sentence[i]) < 2:\n",
        "      sentence_to_remove.append(input_numerical_sentence[i])"
      ],
      "metadata": {
        "id": "QPp1SkAeikPz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentence_to_remove:\n",
        "  input_numerical_sentence.remove(sentence)"
      ],
      "metadata": {
        "id": "Y1Pm9DBwlj-b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence = []\n",
        "\n",
        "for sentence in input_numerical_sentence:\n",
        "  for i in range(1, len(sentence)):\n",
        "    training_sequence.append(sentence[:i+1])"
      ],
      "metadata": {
        "id": "upIc4etZoCbi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_len = []\n",
        "\n",
        "for sentence in training_sequence:\n",
        "  list_len.append(len(sentence))"
      ],
      "metadata": {
        "id": "ISbyEcDrpJHT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math"
      ],
      "metadata": {
        "id": "QwhiffFspr2Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if i do the zero padding while taking 696 as my largest sentence, then i will have alot of zeros in padding causing computational complexity\n",
        "\n",
        "Counter(list_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D7aKgw46uXY9",
        "outputId": "a1d444e3-d4e0-487e-f83e-e4234555f16b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({2: 61292,\n",
              "         3: 57581,\n",
              "         4: 54575,\n",
              "         5: 51735,\n",
              "         6: 49499,\n",
              "         7: 47458,\n",
              "         8: 45826,\n",
              "         9: 44119,\n",
              "         10: 42473,\n",
              "         11: 40551,\n",
              "         12: 37866,\n",
              "         13: 33853,\n",
              "         14: 28890,\n",
              "         15: 23187,\n",
              "         16: 17701,\n",
              "         17: 13088,\n",
              "         18: 9598,\n",
              "         19: 6991,\n",
              "         20: 5189,\n",
              "         21: 3912,\n",
              "         22: 3050,\n",
              "         23: 2349,\n",
              "         24: 1826,\n",
              "         25: 1431,\n",
              "         26: 1149,\n",
              "         27: 948,\n",
              "         28: 795,\n",
              "         29: 660,\n",
              "         30: 562,\n",
              "         31: 471,\n",
              "         32: 409,\n",
              "         33: 348,\n",
              "         34: 297,\n",
              "         35: 251,\n",
              "         36: 216,\n",
              "         37: 183,\n",
              "         38: 167,\n",
              "         39: 153,\n",
              "         40: 138,\n",
              "         41: 126,\n",
              "         42: 116,\n",
              "         43: 110,\n",
              "         44: 96,\n",
              "         45: 86,\n",
              "         46: 83,\n",
              "         47: 77,\n",
              "         48: 76,\n",
              "         49: 70,\n",
              "         50: 59,\n",
              "         51: 58,\n",
              "         52: 51,\n",
              "         53: 45,\n",
              "         54: 41,\n",
              "         55: 34,\n",
              "         56: 32,\n",
              "         57: 28,\n",
              "         58: 26,\n",
              "         59: 25,\n",
              "         60: 24,\n",
              "         61: 23,\n",
              "         62: 22,\n",
              "         63: 22,\n",
              "         64: 21,\n",
              "         65: 21,\n",
              "         66: 21,\n",
              "         67: 21,\n",
              "         68: 20,\n",
              "         69: 20,\n",
              "         70: 19,\n",
              "         71: 17,\n",
              "         72: 15,\n",
              "         73: 15,\n",
              "         74: 12,\n",
              "         75: 11,\n",
              "         76: 11,\n",
              "         77: 11,\n",
              "         78: 11,\n",
              "         79: 11,\n",
              "         80: 11,\n",
              "         81: 11,\n",
              "         82: 11,\n",
              "         83: 11,\n",
              "         84: 11,\n",
              "         85: 11,\n",
              "         86: 9,\n",
              "         87: 9,\n",
              "         88: 9,\n",
              "         89: 9,\n",
              "         90: 9,\n",
              "         91: 7,\n",
              "         92: 7,\n",
              "         93: 7,\n",
              "         94: 7,\n",
              "         95: 7,\n",
              "         96: 7,\n",
              "         97: 7,\n",
              "         98: 7,\n",
              "         99: 7,\n",
              "         100: 7,\n",
              "         101: 7,\n",
              "         102: 7,\n",
              "         103: 7,\n",
              "         104: 7,\n",
              "         105: 7,\n",
              "         106: 7,\n",
              "         107: 7,\n",
              "         108: 7,\n",
              "         109: 7,\n",
              "         110: 7,\n",
              "         111: 7,\n",
              "         112: 7,\n",
              "         113: 7,\n",
              "         114: 7,\n",
              "         115: 7,\n",
              "         116: 7,\n",
              "         117: 7,\n",
              "         118: 7,\n",
              "         119: 7,\n",
              "         120: 7,\n",
              "         121: 7,\n",
              "         122: 7,\n",
              "         123: 7,\n",
              "         124: 7,\n",
              "         125: 7,\n",
              "         126: 6,\n",
              "         127: 6,\n",
              "         128: 5,\n",
              "         129: 5,\n",
              "         130: 5,\n",
              "         131: 5,\n",
              "         132: 5,\n",
              "         133: 5,\n",
              "         134: 5,\n",
              "         135: 5,\n",
              "         136: 5,\n",
              "         137: 5,\n",
              "         138: 5,\n",
              "         139: 5,\n",
              "         140: 5,\n",
              "         141: 5,\n",
              "         142: 5,\n",
              "         143: 5,\n",
              "         144: 5,\n",
              "         145: 5,\n",
              "         146: 5,\n",
              "         147: 5,\n",
              "         148: 5,\n",
              "         149: 5,\n",
              "         150: 5,\n",
              "         151: 5,\n",
              "         152: 5,\n",
              "         153: 5,\n",
              "         154: 5,\n",
              "         155: 5,\n",
              "         156: 5,\n",
              "         157: 5,\n",
              "         158: 5,\n",
              "         159: 5,\n",
              "         160: 5,\n",
              "         161: 5,\n",
              "         162: 5,\n",
              "         163: 5,\n",
              "         164: 5,\n",
              "         165: 5,\n",
              "         166: 5,\n",
              "         167: 5,\n",
              "         168: 5,\n",
              "         169: 4,\n",
              "         170: 4,\n",
              "         171: 4,\n",
              "         172: 4,\n",
              "         173: 4,\n",
              "         174: 4,\n",
              "         175: 4,\n",
              "         176: 4,\n",
              "         177: 4,\n",
              "         178: 4,\n",
              "         179: 4,\n",
              "         180: 4,\n",
              "         181: 4,\n",
              "         182: 4,\n",
              "         183: 4,\n",
              "         184: 4,\n",
              "         185: 4,\n",
              "         186: 4,\n",
              "         187: 4,\n",
              "         188: 4,\n",
              "         189: 4,\n",
              "         190: 4,\n",
              "         191: 4,\n",
              "         192: 4,\n",
              "         193: 4,\n",
              "         194: 4,\n",
              "         195: 4,\n",
              "         196: 4,\n",
              "         197: 4,\n",
              "         198: 4,\n",
              "         199: 4,\n",
              "         200: 4,\n",
              "         201: 4,\n",
              "         202: 4,\n",
              "         203: 4,\n",
              "         204: 4,\n",
              "         205: 4,\n",
              "         206: 4,\n",
              "         207: 4,\n",
              "         208: 4,\n",
              "         209: 4,\n",
              "         210: 4,\n",
              "         211: 4,\n",
              "         212: 4,\n",
              "         213: 4,\n",
              "         214: 4,\n",
              "         215: 4,\n",
              "         216: 4,\n",
              "         217: 4,\n",
              "         218: 4,\n",
              "         219: 4,\n",
              "         220: 4,\n",
              "         221: 4,\n",
              "         222: 4,\n",
              "         223: 4,\n",
              "         224: 4,\n",
              "         225: 4,\n",
              "         226: 4,\n",
              "         227: 4,\n",
              "         228: 4,\n",
              "         229: 4,\n",
              "         230: 4,\n",
              "         231: 4,\n",
              "         232: 4,\n",
              "         233: 4,\n",
              "         234: 4,\n",
              "         235: 4,\n",
              "         236: 4,\n",
              "         237: 4,\n",
              "         238: 4,\n",
              "         239: 4,\n",
              "         240: 4,\n",
              "         241: 4,\n",
              "         242: 4,\n",
              "         243: 4,\n",
              "         244: 4,\n",
              "         245: 4,\n",
              "         246: 4,\n",
              "         247: 4,\n",
              "         248: 4,\n",
              "         249: 4,\n",
              "         250: 4,\n",
              "         251: 4,\n",
              "         252: 4,\n",
              "         253: 4,\n",
              "         254: 4,\n",
              "         255: 4,\n",
              "         256: 4,\n",
              "         257: 4,\n",
              "         258: 4,\n",
              "         259: 4,\n",
              "         260: 4,\n",
              "         261: 4,\n",
              "         262: 4,\n",
              "         263: 4,\n",
              "         264: 4,\n",
              "         265: 4,\n",
              "         266: 4,\n",
              "         267: 4,\n",
              "         268: 4,\n",
              "         269: 4,\n",
              "         270: 4,\n",
              "         271: 4,\n",
              "         272: 4,\n",
              "         273: 4,\n",
              "         274: 4,\n",
              "         275: 3,\n",
              "         276: 3,\n",
              "         277: 3,\n",
              "         278: 3,\n",
              "         279: 3,\n",
              "         280: 3,\n",
              "         281: 3,\n",
              "         282: 3,\n",
              "         283: 3,\n",
              "         284: 3,\n",
              "         285: 3,\n",
              "         286: 3,\n",
              "         287: 3,\n",
              "         288: 3,\n",
              "         289: 3,\n",
              "         290: 3,\n",
              "         291: 3,\n",
              "         292: 3,\n",
              "         293: 3,\n",
              "         294: 3,\n",
              "         295: 3,\n",
              "         296: 3,\n",
              "         297: 3,\n",
              "         298: 3,\n",
              "         299: 3,\n",
              "         300: 3,\n",
              "         301: 3,\n",
              "         302: 3,\n",
              "         303: 3,\n",
              "         304: 3,\n",
              "         305: 3,\n",
              "         306: 3,\n",
              "         307: 3,\n",
              "         308: 3,\n",
              "         309: 3,\n",
              "         310: 3,\n",
              "         311: 3,\n",
              "         312: 3,\n",
              "         313: 3,\n",
              "         314: 3,\n",
              "         315: 3,\n",
              "         316: 3,\n",
              "         317: 3,\n",
              "         318: 3,\n",
              "         319: 3,\n",
              "         320: 3,\n",
              "         321: 3,\n",
              "         322: 3,\n",
              "         323: 3,\n",
              "         324: 3,\n",
              "         325: 3,\n",
              "         326: 3,\n",
              "         327: 3,\n",
              "         328: 3,\n",
              "         329: 3,\n",
              "         330: 3,\n",
              "         331: 3,\n",
              "         332: 3,\n",
              "         333: 3,\n",
              "         334: 3,\n",
              "         335: 3,\n",
              "         336: 3,\n",
              "         337: 3,\n",
              "         338: 3,\n",
              "         339: 3,\n",
              "         340: 3,\n",
              "         341: 3,\n",
              "         342: 3,\n",
              "         343: 3,\n",
              "         344: 3,\n",
              "         345: 3,\n",
              "         346: 3,\n",
              "         347: 3,\n",
              "         348: 3,\n",
              "         349: 3,\n",
              "         350: 3,\n",
              "         351: 3,\n",
              "         352: 3,\n",
              "         353: 3,\n",
              "         354: 3,\n",
              "         355: 3,\n",
              "         356: 3,\n",
              "         357: 3,\n",
              "         358: 3,\n",
              "         359: 3,\n",
              "         360: 3,\n",
              "         361: 3,\n",
              "         362: 3,\n",
              "         363: 3,\n",
              "         364: 3,\n",
              "         365: 3,\n",
              "         366: 3,\n",
              "         367: 3,\n",
              "         368: 3,\n",
              "         369: 3,\n",
              "         370: 3,\n",
              "         371: 3,\n",
              "         372: 3,\n",
              "         373: 3,\n",
              "         374: 3,\n",
              "         375: 3,\n",
              "         376: 3,\n",
              "         377: 3,\n",
              "         378: 3,\n",
              "         379: 3,\n",
              "         380: 3,\n",
              "         381: 3,\n",
              "         382: 3,\n",
              "         383: 3,\n",
              "         384: 3,\n",
              "         385: 3,\n",
              "         386: 3,\n",
              "         387: 3,\n",
              "         388: 3,\n",
              "         389: 3,\n",
              "         390: 3,\n",
              "         391: 3,\n",
              "         392: 3,\n",
              "         393: 3,\n",
              "         394: 3,\n",
              "         395: 3,\n",
              "         396: 3,\n",
              "         397: 3,\n",
              "         398: 3,\n",
              "         399: 3,\n",
              "         400: 3,\n",
              "         401: 3,\n",
              "         402: 3,\n",
              "         403: 3,\n",
              "         404: 3,\n",
              "         405: 3,\n",
              "         406: 3,\n",
              "         407: 3,\n",
              "         408: 3,\n",
              "         409: 3,\n",
              "         410: 3,\n",
              "         411: 3,\n",
              "         412: 3,\n",
              "         413: 3,\n",
              "         414: 3,\n",
              "         415: 3,\n",
              "         416: 3,\n",
              "         417: 3,\n",
              "         418: 3,\n",
              "         419: 3,\n",
              "         420: 3,\n",
              "         421: 3,\n",
              "         422: 3,\n",
              "         423: 3,\n",
              "         424: 3,\n",
              "         425: 3,\n",
              "         426: 3,\n",
              "         427: 3,\n",
              "         428: 3,\n",
              "         429: 3,\n",
              "         430: 3,\n",
              "         431: 3,\n",
              "         432: 3,\n",
              "         433: 3,\n",
              "         434: 2,\n",
              "         435: 2,\n",
              "         436: 2,\n",
              "         437: 2,\n",
              "         438: 2,\n",
              "         439: 2,\n",
              "         440: 2,\n",
              "         441: 2,\n",
              "         442: 2,\n",
              "         443: 2,\n",
              "         444: 2,\n",
              "         445: 2,\n",
              "         446: 2,\n",
              "         447: 2,\n",
              "         448: 2,\n",
              "         449: 2,\n",
              "         450: 2,\n",
              "         451: 2,\n",
              "         452: 2,\n",
              "         453: 2,\n",
              "         454: 2,\n",
              "         455: 2,\n",
              "         456: 2,\n",
              "         457: 2,\n",
              "         458: 2,\n",
              "         459: 2,\n",
              "         460: 2,\n",
              "         461: 2,\n",
              "         462: 2,\n",
              "         463: 2,\n",
              "         464: 2,\n",
              "         465: 2,\n",
              "         466: 1,\n",
              "         467: 1,\n",
              "         468: 1,\n",
              "         469: 1,\n",
              "         470: 1,\n",
              "         471: 1,\n",
              "         472: 1,\n",
              "         473: 1,\n",
              "         474: 1,\n",
              "         475: 1,\n",
              "         476: 1,\n",
              "         477: 1,\n",
              "         478: 1,\n",
              "         479: 1,\n",
              "         480: 1,\n",
              "         481: 1,\n",
              "         482: 1,\n",
              "         483: 1,\n",
              "         484: 1,\n",
              "         485: 1,\n",
              "         486: 1,\n",
              "         487: 1,\n",
              "         488: 1,\n",
              "         489: 1,\n",
              "         490: 1,\n",
              "         491: 1,\n",
              "         492: 1,\n",
              "         493: 1,\n",
              "         494: 1,\n",
              "         495: 1,\n",
              "         496: 1,\n",
              "         497: 1,\n",
              "         498: 1,\n",
              "         499: 1,\n",
              "         500: 1,\n",
              "         501: 1,\n",
              "         502: 1,\n",
              "         503: 1,\n",
              "         504: 1,\n",
              "         505: 1,\n",
              "         506: 1,\n",
              "         507: 1,\n",
              "         508: 1,\n",
              "         509: 1,\n",
              "         510: 1,\n",
              "         511: 1,\n",
              "         512: 1,\n",
              "         513: 1,\n",
              "         514: 1,\n",
              "         515: 1,\n",
              "         516: 1,\n",
              "         517: 1,\n",
              "         518: 1,\n",
              "         519: 1,\n",
              "         520: 1,\n",
              "         521: 1,\n",
              "         522: 1,\n",
              "         523: 1,\n",
              "         524: 1,\n",
              "         525: 1,\n",
              "         526: 1,\n",
              "         527: 1,\n",
              "         528: 1,\n",
              "         529: 1,\n",
              "         530: 1,\n",
              "         531: 1,\n",
              "         532: 1,\n",
              "         533: 1,\n",
              "         534: 1,\n",
              "         535: 1,\n",
              "         536: 1,\n",
              "         537: 1,\n",
              "         538: 1,\n",
              "         539: 1,\n",
              "         540: 1,\n",
              "         541: 1,\n",
              "         542: 1,\n",
              "         543: 1,\n",
              "         544: 1,\n",
              "         545: 1,\n",
              "         546: 1,\n",
              "         547: 1,\n",
              "         548: 1,\n",
              "         549: 1,\n",
              "         550: 1,\n",
              "         551: 1,\n",
              "         552: 1,\n",
              "         553: 1,\n",
              "         554: 1,\n",
              "         555: 1,\n",
              "         556: 1,\n",
              "         557: 1,\n",
              "         558: 1,\n",
              "         559: 1,\n",
              "         560: 1,\n",
              "         561: 1,\n",
              "         562: 1,\n",
              "         563: 1,\n",
              "         564: 1,\n",
              "         565: 1,\n",
              "         566: 1,\n",
              "         567: 1,\n",
              "         568: 1,\n",
              "         569: 1,\n",
              "         570: 1,\n",
              "         571: 1,\n",
              "         572: 1,\n",
              "         573: 1,\n",
              "         574: 1,\n",
              "         575: 1,\n",
              "         576: 1,\n",
              "         577: 1,\n",
              "         578: 1,\n",
              "         579: 1,\n",
              "         580: 1,\n",
              "         581: 1,\n",
              "         582: 1,\n",
              "         583: 1,\n",
              "         584: 1,\n",
              "         585: 1,\n",
              "         586: 1,\n",
              "         587: 1,\n",
              "         588: 1,\n",
              "         589: 1,\n",
              "         590: 1,\n",
              "         591: 1,\n",
              "         592: 1,\n",
              "         593: 1,\n",
              "         594: 1,\n",
              "         595: 1,\n",
              "         596: 1,\n",
              "         597: 1,\n",
              "         598: 1,\n",
              "         599: 1,\n",
              "         600: 1,\n",
              "         601: 1,\n",
              "         602: 1,\n",
              "         603: 1,\n",
              "         604: 1,\n",
              "         605: 1,\n",
              "         606: 1,\n",
              "         607: 1,\n",
              "         608: 1,\n",
              "         609: 1,\n",
              "         610: 1,\n",
              "         611: 1,\n",
              "         612: 1,\n",
              "         613: 1,\n",
              "         614: 1,\n",
              "         615: 1,\n",
              "         616: 1,\n",
              "         617: 1,\n",
              "         618: 1,\n",
              "         619: 1,\n",
              "         620: 1,\n",
              "         621: 1,\n",
              "         622: 1,\n",
              "         623: 1,\n",
              "         624: 1,\n",
              "         625: 1,\n",
              "         626: 1,\n",
              "         627: 1,\n",
              "         628: 1,\n",
              "         629: 1,\n",
              "         630: 1,\n",
              "         631: 1,\n",
              "         632: 1,\n",
              "         633: 1,\n",
              "         634: 1,\n",
              "         635: 1,\n",
              "         636: 1,\n",
              "         637: 1,\n",
              "         638: 1,\n",
              "         639: 1,\n",
              "         640: 1,\n",
              "         641: 1,\n",
              "         642: 1,\n",
              "         643: 1,\n",
              "         644: 1,\n",
              "         645: 1,\n",
              "         646: 1,\n",
              "         647: 1,\n",
              "         648: 1,\n",
              "         649: 1,\n",
              "         650: 1,\n",
              "         651: 1,\n",
              "         652: 1,\n",
              "         653: 1,\n",
              "         654: 1,\n",
              "         655: 1,\n",
              "         656: 1,\n",
              "         657: 1,\n",
              "         658: 1,\n",
              "         659: 1,\n",
              "         660: 1,\n",
              "         661: 1,\n",
              "         662: 1,\n",
              "         663: 1,\n",
              "         664: 1,\n",
              "         665: 1,\n",
              "         666: 1,\n",
              "         667: 1,\n",
              "         668: 1,\n",
              "         669: 1,\n",
              "         670: 1,\n",
              "         671: 1,\n",
              "         672: 1,\n",
              "         673: 1,\n",
              "         674: 1,\n",
              "         675: 1,\n",
              "         676: 1,\n",
              "         677: 1,\n",
              "         678: 1,\n",
              "         679: 1,\n",
              "         680: 1,\n",
              "         681: 1,\n",
              "         682: 1,\n",
              "         683: 1,\n",
              "         684: 1,\n",
              "         685: 1,\n",
              "         686: 1,\n",
              "         687: 1,\n",
              "         688: 1,\n",
              "         689: 1,\n",
              "         690: 1,\n",
              "         691: 1,\n",
              "         692: 1,\n",
              "         693: 1,\n",
              "         694: 1,\n",
              "         695: 1,\n",
              "         696: 1,\n",
              "         697: 1,\n",
              "         698: 1,\n",
              "         699: 1})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_list = []\n",
        "remove_large_sentence = []\n",
        "\n",
        "for i in training_sequence[:]:\n",
        "  if len(i) < 50:\n",
        "    continue\n",
        "  else:\n",
        "    strt = 0\n",
        "    end = 50\n",
        "    remove_large_sentence.append(i)\n",
        "    for j in range(math.ceil(len(i)/50)):\n",
        "      sub_list = i[strt: end]\n",
        "      if len(sub_list) > 2:\n",
        "        split_list.append(sub_list)\n",
        "        strt = end\n",
        "        end = end + 50"
      ],
      "metadata": {
        "id": "R1pT5Eanpxz0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter([len(i) for i in split_list])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sb3gY_r4swb9",
        "outputId": "6fa0af2e-604f-4ce5-8306-bffdaccfc86b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({50: 9777,\n",
              "         3: 80,\n",
              "         4: 76,\n",
              "         5: 69,\n",
              "         6: 67,\n",
              "         7: 63,\n",
              "         8: 61,\n",
              "         9: 60,\n",
              "         10: 59,\n",
              "         11: 58,\n",
              "         12: 57,\n",
              "         13: 57,\n",
              "         14: 56,\n",
              "         15: 56,\n",
              "         16: 55,\n",
              "         17: 55,\n",
              "         18: 54,\n",
              "         19: 53,\n",
              "         20: 52,\n",
              "         21: 50,\n",
              "         22: 48,\n",
              "         23: 48,\n",
              "         24: 45,\n",
              "         25: 43,\n",
              "         26: 42,\n",
              "         27: 42,\n",
              "         28: 41,\n",
              "         29: 41,\n",
              "         30: 41,\n",
              "         31: 41,\n",
              "         32: 41,\n",
              "         33: 41,\n",
              "         34: 40,\n",
              "         35: 40,\n",
              "         36: 38,\n",
              "         37: 38,\n",
              "         38: 38,\n",
              "         39: 38,\n",
              "         40: 38,\n",
              "         41: 36,\n",
              "         42: 36,\n",
              "         43: 36,\n",
              "         44: 36,\n",
              "         45: 36,\n",
              "         46: 36,\n",
              "         47: 36,\n",
              "         48: 36,\n",
              "         49: 36})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in remove_large_sentence:\n",
        "  training_sequence.remove(i)"
      ],
      "metadata": {
        "id": "sfVKsbE8xdOl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in split_list:\n",
        "  training_sequence.append(i)"
      ],
      "metadata": {
        "id": "ChD7L0NGy9Xv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now all the sentence have maximum word of 50.\n",
        "Counter([len(i) for i in training_sequence])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RZViOYUEzTM4",
        "outputId": "a34143eb-b498-40c8-c3bc-b6718bf2b514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({2: 61292,\n",
              "         3: 57661,\n",
              "         4: 54651,\n",
              "         5: 51804,\n",
              "         6: 49566,\n",
              "         7: 47521,\n",
              "         8: 45887,\n",
              "         9: 44179,\n",
              "         10: 42532,\n",
              "         11: 40609,\n",
              "         12: 37923,\n",
              "         13: 33910,\n",
              "         14: 28946,\n",
              "         15: 23243,\n",
              "         16: 17756,\n",
              "         17: 13143,\n",
              "         18: 9652,\n",
              "         19: 7044,\n",
              "         20: 5241,\n",
              "         21: 3962,\n",
              "         22: 3098,\n",
              "         23: 2397,\n",
              "         24: 1871,\n",
              "         25: 1474,\n",
              "         26: 1191,\n",
              "         27: 990,\n",
              "         28: 836,\n",
              "         29: 701,\n",
              "         30: 603,\n",
              "         31: 512,\n",
              "         32: 450,\n",
              "         33: 389,\n",
              "         34: 337,\n",
              "         35: 291,\n",
              "         36: 254,\n",
              "         37: 221,\n",
              "         38: 205,\n",
              "         39: 191,\n",
              "         40: 176,\n",
              "         41: 162,\n",
              "         42: 152,\n",
              "         43: 146,\n",
              "         44: 132,\n",
              "         45: 122,\n",
              "         46: 119,\n",
              "         47: 113,\n",
              "         48: 112,\n",
              "         49: 106,\n",
              "         50: 9777})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making all the sentence of equal length\n",
        "training_sequence = pad_sequences(training_sequence)"
      ],
      "metadata": {
        "id": "NVphtlbu30LY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence = torch.tensor(training_sequence)"
      ],
      "metadata": {
        "id": "sUD5gOV34hfc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = training_sequence[:, :-1]"
      ],
      "metadata": {
        "id": "9kqhLJmK4pV4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = training_sequence[:, -1]"
      ],
      "metadata": {
        "id": "EvXo7lId4_lQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "3htHuvvQ5OJn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(x,y)"
      ],
      "metadata": {
        "id": "-jerK5Ve5c2B"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=100, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "3tFfRsgd58D8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeekLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, 200)\n",
        "    self.lstm = nn.LSTM(200, 256, batch_first=True)\n",
        "    self.fc = nn.Linear(256, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embeddings = self.embedding(x)\n",
        "    intermidiate_hidden_state, (final_hidden_state, final_cell_state) = self.lstm(embeddings)\n",
        "    out = self.fc(final_hidden_state.squeeze(0))\n",
        "    return out"
      ],
      "metadata": {
        "id": "3JbX3nvm6bXE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### understanding flow of tensors"
      ],
      "metadata": {
        "id": "uO77hkhs-veF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batchx, batchy in dataloader:\n",
        "  print(batchx.shape,batchy.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "kq4PNCAk8ThF",
        "outputId": "b4cf4200-55e3-47c1-e05d-635eeaf79fef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 49]) torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batchx.shape # batch of 100 sentence, each sentence has 50 words (0-49)"
      ],
      "metadata": {
        "id": "0OPBVfjP9Xkb",
        "outputId": "c23865a5-b0b9-4988-ed5e-835d9642fc27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = nn.Embedding(len(vocab), 200)\n",
        "\n",
        "xout = x(batchx)\n",
        "\n",
        "# batch of 100 sentence, each sentence has 50 words, and each word is represented using vector embedding of 200 numbers\n",
        "xout.shape"
      ],
      "metadata": {
        "id": "X5mqU9MN7-O6",
        "outputId": "7e12f4a4-a7f3-44af-ce60-8a69ec889f13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 49, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = nn.LSTM(200, 256, batch_first=True)\n",
        "\n",
        "yout = y(xout)[1][0].squeeze(0) # need to squeeze one dim\n",
        "\n",
        "# first word of each 100 sentence, in 256-D LSTM output\n",
        "yout.shape"
      ],
      "metadata": {
        "id": "PV7Vt8ae8MF7",
        "outputId": "37ea82e3-bd1b-4b5a-efb8-b00529380154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = nn.Linear(256, len(vocab))\n",
        "\n",
        "zout = z(yout)\n",
        "\n",
        "# first word of each 100 sentence, in 28320 dim (num of unique sentence)\n",
        "zout.shape"
      ],
      "metadata": {
        "id": "OEDIIWPy8g2Q",
        "outputId": "7ce0997f-c9d9-45c4-ce11-087af834e596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 28321])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GeekLSTM(len(vocab))\n",
        "model = model.to(device)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "epochs = 50\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "yK_MvfFU9DAK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(10):\n",
        "  epoch_loss = []\n",
        "  for batchx, batchy in dataloader:\n",
        "    batchx, batchy = batchx.to(device), batchy.to(device)\n",
        "    output = model(batchx)\n",
        "    loss = loss_function(output, batchy.long())\n",
        "\n",
        "    with torch.no_grad():\n",
        "      epoch_loss.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'in epoch {epoch} loss is {np.mean(epoch_loss)}')"
      ],
      "metadata": {
        "id": "iyMEXhbp_eE-"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, text, vocab = vocab):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    text = text.lower()\n",
        "    numerical_text = text_to_indices(word_tokenize(text))\n",
        "    numerical_text = torch.tensor(numerical_text).unsqueeze(0)\n",
        "    numerical_text = numerical_text.to(device)\n",
        "    output = model(numerical_text)\n",
        "    output = torch.softmax(output, dim=1)\n",
        "    idx = output.argmax(dim=1)\n",
        "    return text + ' ' + list(vocab.keys())[idx]"
      ],
      "metadata": {
        "id": "QnB5AYGOGeaG"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_token = 5\n",
        "\n",
        "input_text = 'normal distribution'\n",
        "\n",
        "for i in range(predict_token):\n",
        "  out = predict(model, input_text)\n",
        "  print(out)\n",
        "  input_text = out"
      ],
      "metadata": {
        "id": "h4aJXmRpXwA4",
        "outputId": "ed530640-422a-44a1-9da6-912a182bbfb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normal distribution with\n",
            "normal distribution with mean\n",
            "normal distribution with mean and\n",
            "normal distribution with mean and variance\n",
            "normal distribution with mean and variance x.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss.item(),\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, 'checkpoint.pth')\n",
        "print(\"Checkpoint saved!\")"
      ],
      "metadata": {
        "id": "OfeNLq-1X0LI",
        "outputId": "26774b8b-791f-44a7-fae0-79f25b382f2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T85bwzD3ePbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}